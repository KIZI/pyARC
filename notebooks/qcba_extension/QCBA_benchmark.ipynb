{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run CAR_creation.ipynb\n",
    "%run ../../main.py\n",
    "\n",
    "import pyarc.qcba as qcba\n",
    "from pyarc import CBA\n",
    "from pyarc.qcba.data_structures import *\n",
    "from pyarc.qcba import QuantitativeClassifier\n",
    "import pyarc.utils.plotting as plotils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyarc.algorithms import M1Algorithm, M2Algorithm, top_rules, createCARs \n",
    "from pyarc.data_structures import TransactionDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarc.qcba.data_structures.quant_dataset.QuantitativeDataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuantitativeDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyarc.qcba import QCBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyarc.qcba.data_structures import (\n",
    "    IntervalReader,\n",
    "    Interval,\n",
    "    QuantitativeDataFrame,\n",
    "    QuantitativeCAR\n",
    ")\n",
    "\n",
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.closed_bracket = \"\", \"NULL\"\n",
    "interval_reader.open_bracket = \"NULL\", \"\"\n",
    "interval_reader.infinity_symbol = \"inf\", \"inf\"\n",
    "interval_reader.members_separator = \"_to_\"\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "i = interval_reader.read(\"82.9815_to_inf\")\n",
    "\n",
    "QuantitativeCAR.interval_reader = interval_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = movies_train_undiscr\n",
    "ds = ds.reset_index()\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"iris\"\n",
    "dataset_index = 1\n",
    "\n",
    "\n",
    "\n",
    "train_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)] \n",
    "test_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "train_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "test_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "dataset_train_undiscr = pd.concat([ pd.read_csv(ds) for ds in train_path_undiscr ])\n",
    "dataset_test_undiscr = pd.concat([ pd.read_csv(ds) for ds in test_path_undiscr ])\n",
    "dataset_test_undiscr_Y = dataset_test_undiscr.iloc[:,-1]\n",
    "\n",
    "quant_dataset_train = QuantitativeDataFrame(dataset_train_undiscr)\n",
    "quant_dataset_test = QuantitativeDataFrame(dataset_test_undiscr)\n",
    "\n",
    "txns_train_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in train_path_discr]))\n",
    "txns_test_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in test_path_discr]))\n",
    "\n",
    "rm_cba = CBA(algorithm=\"m1\", confidence=0.1, support=0.01).fit(txns_train_discr, top_rules_args={\"target_rule_count\":1000})\n",
    "\n",
    "rm_qcba = QCBA(rm_cba, quant_dataset_train)\n",
    "rm_qcba.fit(\n",
    "    refitting=True,\n",
    "    literal_pruning=True,\n",
    "    trimming=True,\n",
    "    extension=True,\n",
    "    overlap_pruning=True,\n",
    "    transaction_based_drop=True\n",
    ")\n",
    "\n",
    "\n",
    "rm_qcba.clf.rule_model_accuracy(quant_dataset_test, dataset_test_undiscr_Y), rm_cba.rule_model_accuracy(txns_test_discr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.closed_bracket = \"\", \"NULL\"\n",
    "interval_reader.open_bracket = \"NULL\", \"\"\n",
    "interval_reader.infinity_symbol = \"inf\", \"inf\"\n",
    "interval_reader.members_separator = \"_to_\"\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "i = interval_reader.read(\"82.9815_to_inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarc.qcba.transformation import QCBATransformation\n",
    "\n",
    "dataset_name = \"iris\"\n",
    "dataset_index = 1\n",
    "\n",
    "\n",
    "\n",
    "train_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)] \n",
    "test_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "train_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "test_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "dataset_train_undiscr = pd.concat([ pd.read_csv(ds) for ds in train_path_undiscr ])\n",
    "dataset_test_undiscr = pd.concat([ pd.read_csv(ds) for ds in test_path_undiscr ])\n",
    "dataset_test_undiscr_Y = dataset_test_undiscr.iloc[:,-1]\n",
    "\n",
    "quant_dataset_train = QuantitativeDataFrame(dataset_train_undiscr)\n",
    "quant_dataset_test = QuantitativeDataFrame(dataset_test_undiscr)\n",
    "\n",
    "txns_train_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in train_path_discr]))\n",
    "txns_test_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in test_path_discr]))\n",
    "\n",
    "rm_cba = CBA(algorithm=\"m1\", confidence=0.1, support=0.01).fit(txns_train_discr)\n",
    "\n",
    "cba_rule_model = rm_cba\n",
    "quantitative_dataset = quant_dataset\n",
    "\n",
    "__quant_rules = [ QuantitativeCAR(r) for r in cba_rule_model.clf.rules ] \n",
    "\n",
    "qcba_transformation = QCBATransformation(quant_dataset_train)\n",
    "\n",
    "\n",
    "refitting=True,\n",
    "literal_pruning=True,\n",
    "trimming=True,\n",
    "extension=True,\n",
    "overlap_pruning=True,\n",
    "transaction_based_drop=True\n",
    "\n",
    "transformation_dict = {\n",
    "    \"refitting\": refitting,\n",
    "    \"literal_pruning\": literal_pruning,\n",
    "    \"trimming\": trimming,\n",
    "    \"extension\": extension,\n",
    "    \"overlap_pruning\": overlap_pruning,\n",
    "    \"transaction_based_drop\": transaction_based_drop\n",
    "}\n",
    "\n",
    "\n",
    "qcba_transformation.transform(__quant_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class RuleExtender1:\n",
    "    \n",
    "    def __init__(self, dataframe, min_conditional_improvement=-0.01, min_improvement=0):\n",
    "    \n",
    "        if type(dataframe) != QuantitativeDataFrame:\n",
    "            raise Exception(\n",
    "                \"type of dataset must be pandas.DataFrame\"\n",
    "            )\n",
    "            \n",
    "        self.__dataframe = dataframe\n",
    "        self.min_conditional_improvement = min_conditional_improvement\n",
    "        self.min_improvement = min_improvement\n",
    "        \n",
    "        \n",
    "    def transform_greedy(self, rules, skip_ahead=1):\n",
    "        \n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        progress_bar_len = 50\n",
    "        copied_rules_len = len(copied_rules)\n",
    "        progress_bar = \"#\" * progress_bar_len\n",
    "        progress_bar_empty = \" \" * progress_bar_len\n",
    "        last_progress_bar_idx = -1\n",
    "\n",
    "        extended_rules = []\n",
    "\n",
    "        #print(\"len: \", copied_rules_len)\n",
    "\n",
    "        for i, rule in enumerate(copied_rules):\n",
    "            current_progress_bar_idx = math.floor(i / copied_rules_len * progress_bar_len)\n",
    "            \n",
    "            if last_progress_bar_idx != current_progress_bar_idx:\n",
    "                last_progress_bar_idx = current_progress_bar_idx\n",
    "                \n",
    "                progress_string = \"[\" + progress_bar[:last_progress_bar_idx] + progress_bar_empty[last_progress_bar_idx:] + \"]\"\n",
    "                \n",
    "                print(*progress_string, sep=\"\")\n",
    "\n",
    "            extended_rules.append(self.__extend_greedy(rule, skip_ahead=skip_ahead))\n",
    "        \n",
    "        return extended_rules\n",
    "    \n",
    "    \n",
    "        \n",
    "    def __extend_greedy(self, rule, skip_ahead=1):\n",
    "        ext = self.__extend_rule_greedy(rule, skip_ahead=skip_ahead)\n",
    "        \n",
    "        return ext\n",
    "    \n",
    "    \n",
    "    def __extend_rule_greedy(self, rule, skip_ahead=1):\n",
    "        \n",
    "        \n",
    "        min_improvement=self.min_improvement\n",
    "        min_conditional_improvement=self.min_conditional_improvement\n",
    "        \n",
    "        # check improvemnt argument ranges\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        current_best = rule\n",
    "        direct_extensions = self.__get_extensions_greedy(rule)\n",
    "        \n",
    "        current_best.update_properties(self.__dataframe)\n",
    "        \n",
    "        while True:\n",
    "            extension_succesful = False\n",
    "\n",
    "            direct_extensions = self.__get_extensions_greedy(current_best)\n",
    "\n",
    "            #print(\"extending - new cycle\")\n",
    "            \n",
    "            for candidate in direct_extensions:\n",
    "                #print(\"\\tcandidate - direct extensions\")\n",
    "                #print(\"\\t\", direct_extensions)\n",
    "                candidate.update_properties(self.__dataframe)\n",
    "                \n",
    "                delta_confidence = candidate.confidence - current_best.confidence\n",
    "                delta_support = candidate.support - current_best.support\n",
    "                \n",
    "                \n",
    "                if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                    current_best = candidate\n",
    "                    extension_succesful = True\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                if self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                    enlargement = candidate\n",
    "                    \n",
    "                    while True:\n",
    "                        enlargement = self.get_beam_extensions_greedy(enlargement, skip_ahead=skip_ahead)\n",
    "                        \n",
    "                        if not enlargement:\n",
    "                            break\n",
    "                            \n",
    "                        candidate.update_properties(self.__dataframe)\n",
    "                        enlargement.update_properties(self.__dataframe)\n",
    "\n",
    "                        delta_confidence = enlargement.confidence - current_best.confidence\n",
    "                        delta_support = enlargement.support - current_best.support\n",
    "\n",
    "                        if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                            current_best = enlargement\n",
    "                            #plotils.plot_quant_rules([current_best])\n",
    "                            #plt.show()\n",
    "                            \n",
    "                            #print(step)\n",
    "                            step += 1\n",
    "                            \n",
    "                            extension_succesful = True\n",
    "                            \n",
    "                        elif self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                            #plotils.plot_quant_rules([enlargement])\n",
    "                            #plt.show()\n",
    "                            \n",
    "                            #print(step)\n",
    "                            step += 1\n",
    "                            \n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            break\n",
    "            \n",
    "            \n",
    "                    if extension_succesful == True:\n",
    "                        break\n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    # continue to next candidate\n",
    "                    continue\n",
    "           \n",
    "        \n",
    "            if extension_succesful == False:\n",
    "                break\n",
    "                    \n",
    "        return current_best\n",
    "    \n",
    "    \n",
    "            \n",
    "    def __get_extensions_greedy(self, rule, skip_ahead=1):\n",
    "        extended_rules = []\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            neighborhood = self.__get_direct_extensions_greedy(literal, skip_ahead=skip_ahead)\n",
    "            \n",
    "            for extended_literal in neighborhood:\n",
    "                # copy the rule so the extended literal\n",
    "                # can replace the default literal\n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                # find the index of the literal\n",
    "                # so that it can be replaced\n",
    "                current_literal_index = copied_rule.antecedent.index(literal)\n",
    "                \n",
    "                copied_rule.antecedent[current_literal_index] = extended_literal\n",
    "                copied_rule.was_extended = True\n",
    "                copied_rule.extended_literal = extended_literal\n",
    "                \n",
    "                extended_rules.append(copied_rule)\n",
    "\n",
    "        extended_rules.sort(reverse=True)\n",
    "             \n",
    "        return extended_rules\n",
    "        \n",
    "        \n",
    "   \n",
    "    def __get_direct_extensions_greedy(self, literal, skip_ahead=1):\n",
    "        \"\"\"\n",
    "        ensure sort and unique\n",
    "        before calling functions\n",
    "        \"\"\"\n",
    "        \n",
    "        attribute, interval = literal\n",
    "\n",
    "        # if nominal\n",
    "        # needs correction to return null and skip when extending\n",
    "        if type(interval) == str:\n",
    "            return [literal]\n",
    "        \n",
    "        vals = self.__dataframe.column(attribute)\n",
    "        vals_len = vals.size\n",
    "\n",
    "        mask = interval.test_membership(vals)\n",
    "\n",
    "        # indices of interval members\n",
    "        # we want to extend them \n",
    "        # once to the left\n",
    "        # and once to the right\n",
    "        # bu we have to check if resulting\n",
    "        # indices are not larger than value size\n",
    "        member_indexes = np.where(mask)[0]\n",
    "\n",
    "        first_index = member_indexes[0]\n",
    "        last_index = member_indexes[-1]\n",
    "\n",
    "        first_index_modified = first_index - skip_ahead\n",
    "        last_index_modified = last_index + skip_ahead\n",
    "        \n",
    "        no_left_extension = False\n",
    "        no_right_extension = False\n",
    "\n",
    "        if first_index_modified < 0:\n",
    "            no_left_extension = True\n",
    "\n",
    "        # if last_index_modified is larger than\n",
    "        # available indices\n",
    "        if last_index_modified > vals_len - 1:\n",
    "            no_right_extension = True\n",
    "\n",
    "\n",
    "        new_left_bound = interval.minval\n",
    "        new_right_bound = interval.maxval\n",
    "\n",
    "        if not no_left_extension:\n",
    "            new_left_bound = vals[first_index_modified]\n",
    "\n",
    "        if not no_right_extension:\n",
    "            new_right_bound = vals[last_index_modified]\n",
    "\n",
    "\n",
    "        # prepare return values\n",
    "        extensions = []\n",
    "\n",
    "        if not no_left_extension:\n",
    "            # when values are [1, 2, 3, 3, 4, 5]\n",
    "            # and the corresponding interval is (2, 4)\n",
    "            # instead of resulting interval being (1, 4)\n",
    "            \n",
    "            temp_interval = Interval(\n",
    "                new_left_bound,\n",
    "                interval.maxval,\n",
    "                True,\n",
    "                interval.right_inclusive\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        if not no_right_extension:\n",
    "\n",
    "            temp_interval = Interval(\n",
    "                interval.minval,\n",
    "                new_right_bound,\n",
    "                interval.left_inclusive,\n",
    "                True\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        return extensions\n",
    "        \n",
    "    \n",
    "    # make private\n",
    "    def get_beam_extensions_greedy(self, rule, skip_ahead=1):\n",
    "        if not rule.was_extended:\n",
    "            return None\n",
    "\n",
    "        # literal which extended the rule\n",
    "        literal = rule.extended_literal\n",
    "        \n",
    "        extended_literal = self.__get_direct_extensions_greedy(literal, skip_ahead=skip_ahead)\n",
    "        \n",
    "        if not extended_literal and skip_ahead > 1:\n",
    "            return self.get_beam_extensions_greedy(rule, skip_ahead=1)\n",
    "        elif not extended_literal:\n",
    "            return None\n",
    "        \n",
    "        copied_rule = rule.copy()\n",
    "        \n",
    "        literal_index = copied_rule.antecedent.index(literal)\n",
    "        \n",
    "        # so that literal is not an array\n",
    "        copied_rule.antecedent[literal_index] = extended_literal[0]\n",
    "        copied_rule.was_extended = True\n",
    "        copied_rule.extended_literal = extended_literal[0]\n",
    "        \n",
    "        return copied_rule\n",
    "        \n",
    "    \n",
    "    def __crisp_accept(self, delta_confidence, delta_support, min_improvement):\n",
    "        if delta_confidence >= min_improvement and delta_support > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __conditional_accept(self, delta_conf, min_improvement):\n",
    "        if delta_conf >= min_improvement:\n",
    "            return True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarc.qcba.transformation import *\n",
    "\n",
    "ir = IntervalReader()\n",
    "QuantitativeCAR.interval_reader = ir\n",
    "\n",
    "movies_train_undiscr = pd.read_csv(\"../data/movies.csv\", sep=\";\", index_col=0)\n",
    "movies_train_discr = pd.read_csv(\"../data/movies_discr.csv\", sep=\";\", index_col=0)\n",
    "\n",
    "movies_undiscr_txns = movies_train_undiscr.reset_index()\n",
    "movies_discr_txns = TransactionDB.from_DataFrame(movies_train_discr)\n",
    "\n",
    "rm = CBA(algorithm=\"m1\", confidence=0.2, support=0.02).fit(movies_discr_txns)\n",
    "\n",
    "rules = rm.clf.rules\n",
    "\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ]\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "#extended_rules = qcba_transformation.extender.transform(quant_rules)\n",
    "quant_rules\n",
    "\n",
    "\n",
    "\n",
    "#rule_extender.transform(quant_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rule_extender = RuleExtender1(quant_dataset, min_conditional_improvement=-0.01)\n",
    "rule_extender_original = RuleExtender(quant_dataset)\n",
    "\n",
    "qrule_to_extend = quant_rules[0].copy()\n",
    "qrule_to_extend.antecedent = [\n",
    "    (\"a-list-celebrities\", Interval(2.5, 3.5, True, True)),\n",
    "    (\"estimated-budget\", Interval(60, 140, True, True))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "qrules_extended = rule_extender.transform_greedy([qrule_to_extend], skip_ahead=1)\n",
    "#qrules_extended = rule_extender_original.transform([qrule_to_extend])\n",
    "plotils.plot_quant_rules(qrules_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refitted = qcba_transformation.refitter.transform(quant_rules)\n",
    "literal_pruned = qcba_transformation.literal_pruner.transform(refitted)\n",
    "trimmed = qcba_transformation.trimmer.transform(literal_pruned)\n",
    "extended = qcba_transformation.extender.transform(trimmed)\n",
    "#extended = rule_extender.transform_greedy(trimmed, skip_ahead=2)\n",
    "post_pruned, default_class = qcba_transformation.post_pruner.transform(extended)\n",
    "overlap_pruned = qcba_transformation.overlap_pruner.transform(post_pruned, default_class)\n",
    "\n",
    "\n",
    "clf = QuantitativeClassifier(overlap_pruned, default_class)\n",
    "\n",
    "clf.rule_model_accuracy(QuantitativeDataFrame(movies_train_undiscr), movies_train_undiscr.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_list = np.array([\"Ahoj\", \"ne\", \"ahoj\", \"Ahoj\", \"nnn\", \"jjjje\"])\n",
    "\n",
    "string_list == \"Ahoj\"\n",
    "\n",
    "\n",
    "string_list == \"Ahoj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quant_rules_iris = [ QuantitativeCAR(r) for r in rm_cba.clf.rules ]\n",
    "quant_dataset_train\n",
    "\n",
    "qcba_transformation_iris = QCBATransformation(quant_dataset_train)\n",
    "refitted = qcba_transformation_iris.refitter.transform(quant_rules_iris)\n",
    "literal_pruned = qcba_transformation_iris.literal_pruner.transform(refitted)\n",
    "trimmed = qcba_transformation_iris.trimmer.transform(literal_pruned)\n",
    "extended = qcba_transformation_iris.extender.transform(trimmed)\n",
    "post_pruned, default_class = qcba_transformation_iris.post_pruner.transform(extended)\n",
    "overlap_pruned = qcba_transformation_iris.overlap_pruner.transform(post_pruned, default_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname = \"iris0\"\n",
    "\n",
    "pd_ds = pd.read_csv(\"c:/code/python/machine_learning/assoc_rules/train/{}.csv\".format(datasetname))\n",
    "    \n",
    "    \n",
    "pd_ds_undiscr = pd.read_csv(\"c:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}.csv\".format(datasetname))\n",
    "pd_ds_undiscr_test = pd.read_csv(\"c:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}.csv\".format(datasetname))\n",
    "\n",
    "txns = TransactionDB.from_DataFrame(pd_ds)\n",
    "txns_test = TransactionDB.from_DataFrame(pd.read_csv(\"c:/code/python/machine_learning/assoc_rules/test/{}.csv\".format(datasetname)))\n",
    "\n",
    "\n",
    "rm_cba = CBA()\n",
    "rm_cba.fit(txns)\n",
    "\n",
    "rm_qcba = QCBA(rm_cba, QuantitativeDataFrame(pd_ds_undiscr))\n",
    "qcba_clf = rm_qcba.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
