{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../../main.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyarc.algorithms import M1Algorithm, M2Algorithm, top_rules, createCARs \n",
    "from pyarc.data_structures import TransactionDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 260\n",
      "M1 duration: 0.034238791465759276\n",
      "M1 unique duration 0.026332902908325195\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 520\n",
      "M1 duration: 0.05801247755686442\n",
      "M1 unique duration 0.04233460426330567\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 1040\n",
      "M1 duration: 0.14171892801920574\n",
      "M1 unique duration 0.06253048578898111\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 2080\n",
      "M1 duration: 0.29518922964731853\n",
      "M1 unique duration 0.16332671642303467\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 4160\n",
      "M1 duration: 0.45167106787363687\n",
      "M1 unique duration 0.3186583121617635\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 8320\n",
      "M1 duration: 1.0850087086359659\n",
      "M1 unique duration 1.5088965177536011\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 16640\n",
      "M1 duration: 2.5601552009582518\n",
      "M1 unique duration 1.5503385305404662\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 33280\n",
      "M1 duration: 4.910679125785828\n",
      "M1 unique duration 4.075005586942037\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n",
      "data_count: 66560\n",
      "M1 duration: 10.749090051651\n",
      "M1 unique duration 7.249671292304993\n",
      "M1 output rules 25.0\n",
      "\n",
      "\n",
      "\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=19\n",
      "Rule count: 2266, Iteration: 1\n",
      "Target rule count satisfied: 100\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# =========================\n",
    "# Oveření běhu v závislosti na vložených pravidlech / instancích\n",
    "# =========================\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "import time\n",
    "\n",
    "rule_count = 100\n",
    "\n",
    "benchmark_data = {\n",
    "    \"input rows\": [],\n",
    "    \"input rules\": [],\n",
    "    \"output rules M1 pyARC\": [],\n",
    "    \"output rules M1 pyARC unique\": [],\n",
    "    \"output rules M2 pyARC\": [],\n",
    "    \"time M1 pyARC\": [],\n",
    "    \"time M1 pyARC unique\": [],\n",
    "    \"time M2 pyARC\": []\n",
    "}\n",
    "\n",
    "stop_m2 = True\n",
    "\n",
    "number_of_iterations = 30\n",
    "\n",
    "directory = \"c:/code/python/machine_learning/assoc_rules\"\n",
    "\n",
    "dataset_name_benchmark = \"lymph0\"\n",
    "\n",
    "pd_ds = pd.read_csv(\"c:/code/python/machine_learning/assoc_rules/train/{}.csv\".format(dataset_name_benchmark))\n",
    "\n",
    "for i in range(11):\n",
    "    dataset_name_benchmark = \"lymph0\"\n",
    "    \n",
    "    pd_ds = pd.concat([pd_ds, pd_ds])\n",
    "    \n",
    "    txns = TransactionDB.from_DataFrame(pd_ds, unique_transactions=True)\n",
    "    txns_unique = TransactionDB.from_DataFrame(pd_ds, unique_transactions=False) \n",
    "    \n",
    "    rules = top_rules(txns.string_representation, appearance=txns.appeardict, target_rule_count=rule_count)\n",
    "\n",
    "    cars = createCARs(rules)\n",
    "     \n",
    "    if len(cars) > rule_count:\n",
    "        cars = cars[:rule_count]    \n",
    "\n",
    "        \n",
    "    m1t1 = time.time()\n",
    "    m1clf_len = []\n",
    "    for _ in range(number_of_iterations):\n",
    "        m1 = M1Algorithm(cars, txns)\n",
    "        clf = m1.build()\n",
    "        m1clf_len.append(len(clf.rules) + 1)\n",
    "    \n",
    "    m1t2 = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    m1t1_unique = time.time()\n",
    "    m1clf_len_unique = []\n",
    "    for _ in range(number_of_iterations):\n",
    "        m1 = M1Algorithm(cars, txns_unique)\n",
    "        clf = m1.build()\n",
    "        m1clf_len_unique.append(len(clf.rules) + 1)\n",
    "    \n",
    "    m1t2_unique = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not stop_m2:\n",
    "        m2t1 = time.time()\n",
    "        m2clf_len = []\n",
    "        for _ in range(number_of_iterations):\n",
    "            m2 = M2Algorithm(cars, txns)\n",
    "            clf = m2.build()\n",
    "            m2clf_len.append(len(clf.rules) + 1)\n",
    "\n",
    "        m2t2 = time.time()\n",
    "    \n",
    "     \n",
    "    m1duration = (m1t2 - m1t1) / number_of_iterations\n",
    "    m1duration_unique = (m1t2_unique - m1t1_unique) / number_of_iterations\n",
    "    outputrules_m1 = sum(m1clf_len) / len(m1clf_len)\n",
    "    outputrules_m1_unique = sum(m1clf_len_unique) / len(m1clf_len_unique)\n",
    "    \n",
    "    if not stop_m2:\n",
    "        m2duration = (m2t2 - m2t1) / number_of_iterations\n",
    "        outputrules_m2 = sum(m2clf_len) / len(m2clf_len)\n",
    "        if m2duration > 0.5:\n",
    "            stop_m2 = True\n",
    "    \n",
    "    benchmark_data[\"input rows\"].append(len(txns))\n",
    "    benchmark_data[\"input rules\"].append(rule_count)\n",
    "    benchmark_data[\"output rules M1 pyARC\"].append(outputrules_m1)\n",
    "    benchmark_data[\"output rules M1 pyARC unique\"].append(outputrules_m1_unique)\n",
    "    benchmark_data[\"output rules M2 pyARC\"].append(None if stop_m2 else outputrules_m2)\n",
    "    benchmark_data[\"time M1 pyARC\"].append(m1duration)\n",
    "    benchmark_data[\"time M1 pyARC unique\"].append(m1duration_unique)\n",
    "    benchmark_data[\"time M2 pyARC\"].append(None if stop_m2 else m2duration)\n",
    "\n",
    "    print(\"data_count:\", len(txns))\n",
    "    print(\"M1 duration:\", m1duration)\n",
    "    print(\"M1 unique duration\", m1duration_unique)\n",
    "    print(\"M1 output rules\", outputrules_m1)\n",
    "    if not stop_m2:\n",
    "        print(\"M2 duration:\", m2duration)\n",
    "        print(\"M2 output rules\", outputrules_m2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#benchmark_data.pop(\"M2_duration\")\n",
    "\n",
    "benchmark_df = pd.DataFrame(benchmark_data)\n",
    "\n",
    "benchmark_df.plot(x=[\"input rows\"], y=[\"time M1 pyARC\", \"time M2 pyARC\"])\n",
    "\n",
    "#benchmark_df.to_csv(\"../data/data_sensitivity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_benchmark = pd.read_csv(\"../data/arc-data-size.csv\")\n",
    "\n",
    "R_benchmark[[\"input rows\"]] = R_benchmark[[\"input rows\"]].astype(str)\n",
    "R_benchmark.set_index(\"input rows\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_df[[\"input rows\"]] = benchmark_df[[\"input rows\"]].astype(str)\n",
    "benchmark_df = benchmark_df.set_index(\"input rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_all = benchmark_df.join(R_benchmark, lsuffix=\"_py\", rsuffix=\"_R\")\n",
    "benchmark_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"pyARC - m1\", \"pyARC - m2\", \"arc\", \"rCBA\", \"arulesCBA\"]\n",
    "\n",
    "ax = benchmark_all.plot(y=[\"time M1 pyARC\", \"time M2 pyARC\", \"time_arc\", \"time_acba\", \"time_rcba\"])\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.savefig(\"../data/data_size_sensitivity_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_all.plot(y=[\"time M1 pyARC\", \"time M2 pyARC\", \"time_arc\", \"time_acba\", \"time_rcba\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
