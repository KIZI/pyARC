{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run CAR_creation.ipynb\n",
    "%run ../../main.py\n",
    "\n",
    "import pyarc.qcba as qcba\n",
    "from pyarc import CBA\n",
    "from pyarc.qcba.data_structures import *\n",
    "import pyarc.utils.plotting as plotils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyarc.qcba import QCBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyarc.qcba.data_structures import (\n",
    "    IntervalReader,\n",
    "    Interval,\n",
    "    QuantitativeDataFrame,\n",
    "    QuantitativeCAR\n",
    ")\n",
    "\n",
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.closed_bracket = \"\", \"NULL\"\n",
    "interval_reader.open_bracket = \"NULL\", \"\"\n",
    "interval_reader.infinity_symbol = \"inf\", \"inf\"\n",
    "interval_reader.members_separator = \"_to_\"\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "i = interval_reader.read(\"82.9815_to_inf\")\n",
    "\n",
    "QuantitativeCAR.interval_reader = interval_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CAR {a-list-celebrities=<0;2)} => {class=box-office-bomb} sup: 0.31 conf: 1.00 len: 2, id: 89,\n",
       " CAR {estimated-budget=<250;300)} => {class=main-stream-hit} sup: 0.06 conf: 1.00 len: 2, id: 51,\n",
       " CAR {a-list-celebrities=<4;6),estimated-budget=<0;50)} => {class=critical-success} sup: 0.06 conf: 1.00 len: 3, id: 71,\n",
       " CAR {a-list-celebrities=<6;8)} => {class=critical-success} sup: 0.03 conf: 1.00 len: 2, id: 48,\n",
       " CAR {a-list-celebrities=<4;6),estimated-budget=<100;150)} => {class=main-stream-hit} sup: 0.03 conf: 1.00 len: 3, id: 57,\n",
       " CAR {a-list-celebrities=<4;6),estimated-budget=<150;200)} => {class=main-stream-hit} sup: 0.03 conf: 1.00 len: 3, id: 63,\n",
       " CAR {estimated-budget=<200;250)} => {class=box-office-bomb} sup: 0.06 conf: 0.67 len: 2, id: 56,\n",
       " CAR {estimated-budget=<0;50)} => {class=box-office-bomb} sup: 0.14 conf: 0.62 len: 2, id: 76,\n",
       " CAR {a-list-celebrities=<4;6)} => {class=main-stream-hit} sup: 0.11 conf: 0.50 len: 2, id: 80]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = movies_train_undiscr\n",
    "ds = ds.reset_index()\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "Y = ds[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=5\n",
      "Rule count: 403, Iteration: 1\n",
      "Increasing maxlen 4\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=4, MAX_RULE_LEN=5\n",
      "Rule count: 809, Iteration: 2\n",
      "Increasing maxlen 5\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 3\n",
      "Decreasing confidence to 0.45\n",
      "Running apriori with setting: confidence=0.45, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 4\n",
      "Decreasing confidence to 0.4\n",
      "Running apriori with setting: confidence=0.4, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 5\n",
      "Decreasing confidence to 0.35000000000000003\n",
      "Running apriori with setting: confidence=0.35000000000000003, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 6\n",
      "Decreasing confidence to 0.30000000000000004\n",
      "Running apriori with setting: confidence=0.30000000000000004, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 7\n",
      "Decreasing confidence to 0.25000000000000006\n",
      "Running apriori with setting: confidence=0.25000000000000006, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 8\n",
      "Decreasing confidence to 0.20000000000000007\n",
      "Running apriori with setting: confidence=0.20000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 9\n",
      "Decreasing confidence to 0.15000000000000008\n",
      "Running apriori with setting: confidence=0.15000000000000008, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 10\n",
      "Decreasing confidence to 0.10000000000000007\n",
      "Running apriori with setting: confidence=0.10000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 11\n",
      "Decreasing confidence to 0.05000000000000007\n",
      "Running apriori with setting: confidence=0.05000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 12\n",
      "Decreasing confidence to 6.938893903907228e-17\n",
      "Running apriori with setting: confidence=6.938893903907228e-17, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 13\n",
      "All options exhausted\n",
      "applying selected transformations\n",
      "refitting\n",
      "literal pruning\n",
      "trimming\n",
      "extending\n",
      "[                                                  ]\n",
      "[#                                                 ]\n",
      "[###                                               ]\n",
      "[#####                                             ]\n",
      "[######                                            ]\n",
      "[########                                          ]\n",
      "[##########                                        ]\n",
      "[############                                      ]\n",
      "[#############                                     ]\n",
      "[###############                                   ]\n",
      "[#################                                 ]\n",
      "[##################                                ]\n",
      "[####################                              ]\n",
      "[######################                            ]\n",
      "[########################                          ]\n",
      "[#########################                         ]\n",
      "[###########################                       ]\n",
      "[#############################                     ]\n",
      "[###############################                   ]\n",
      "[################################                  ]\n",
      "[##################################                ]\n",
      "[####################################              ]\n",
      "[#####################################             ]\n",
      "[#######################################           ]\n",
      "[#########################################         ]\n",
      "[###########################################       ]\n",
      "[############################################      ]\n",
      "[##############################################    ]\n",
      "[################################################  ]\n",
      "post pruning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e247094d37d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mextension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0moverlap_pruning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mtransaction_based_drop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\qcba.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, refitting, literal_pruning, trimming, extension, overlap_pruning, transaction_based_drop)\u001b[0m\n\u001b[0;32m     34\u001b[0m         }\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mtransformed_rules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcba_transformation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__quant_rules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformation_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQuantitativeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_rules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transformation.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, rules, transformation_dict)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"post pruning\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mtransformed_rules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_pruner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_rules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtransformation_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overlap_pruning\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\postprune.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, rules)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcopied_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mpruned_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopied_rules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpruned_rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\postprune.py\u001b[0m in \u001b[0;36mprune\u001b[1;34m(self, rules)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mclass_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mdefault_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_class_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_most_frequent_from_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;31m# don't forget to update dataset length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\postprune.py\u001b[0m in \u001b[0;36mget_most_frequent_from_numpy\u001b[1;34m(self, ndarray)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mmaxpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmaxpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmaxpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "dataset_name = \"iris\"\n",
    "dataset_index = 1\n",
    "\n",
    "\n",
    "\n",
    "train_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)] \n",
    "test_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "train_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "test_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "dataset_train_undiscr = pd.concat([ pd.read_csv(ds) for ds in train_path_undiscr ])\n",
    "dataset_test_undiscr = pd.concat([ pd.read_csv(ds) for ds in test_path_undiscr ])\n",
    "dataset_test_undiscr_Y = dataset_test_undiscr.iloc[:,-1]\n",
    "\n",
    "quant_dataset_train = QuantitativeDataFrame(dataset_train_undiscr)\n",
    "quant_dataset_test = QuantitativeDataFrame(dataset_test_undiscr)\n",
    "\n",
    "txns_train_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in train_path_discr]))\n",
    "txns_test_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in test_path_discr]))\n",
    "\n",
    "rm_cba = CBA(algorithm=\"m1\", confidence=0.1, support=0.01).fit(txns_train_discr, top_rules_args={\"target_rule_count\":1000})\n",
    "\n",
    "rm_qcba = QCBA(rm_cba, quant_dataset_train)\n",
    "rm_qcba.fit(\n",
    "    refitting=True,\n",
    "    literal_pruning=True,\n",
    "    trimming=True,\n",
    "    extension=True,\n",
    "    overlap_pruning=True,\n",
    "    transaction_based_drop=True\n",
    ")\n",
    "\n",
    "\n",
    "rm_qcba.clf.rule_model_accuracy(quant_dataset_test, dataset_test_undiscr_Y), rm_cba.rule_model_accuracy(txns_test_discr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_reader = IntervalReader()\n",
    "\n",
    "interval_reader.closed_bracket = \"\", \"NULL\"\n",
    "interval_reader.open_bracket = \"NULL\", \"\"\n",
    "interval_reader.infinity_symbol = \"inf\", \"inf\"\n",
    "interval_reader.members_separator = \"_to_\"\n",
    "\n",
    "interval_reader.compile_reader()\n",
    "\n",
    "i = interval_reader.read(\"82.9815_to_inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=3, MAX_RULE_LEN=5\n",
      "Rule count: 403, Iteration: 1\n",
      "Increasing maxlen 4\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=4, MAX_RULE_LEN=5\n",
      "Rule count: 809, Iteration: 2\n",
      "Increasing maxlen 5\n",
      "Running apriori with setting: confidence=0.5, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 3\n",
      "Decreasing confidence to 0.45\n",
      "Running apriori with setting: confidence=0.45, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 4\n",
      "Decreasing confidence to 0.4\n",
      "Running apriori with setting: confidence=0.4, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 5\n",
      "Decreasing confidence to 0.35000000000000003\n",
      "Running apriori with setting: confidence=0.35000000000000003, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 6\n",
      "Decreasing confidence to 0.30000000000000004\n",
      "Running apriori with setting: confidence=0.30000000000000004, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 7\n",
      "Decreasing confidence to 0.25000000000000006\n",
      "Running apriori with setting: confidence=0.25000000000000006, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 8\n",
      "Decreasing confidence to 0.20000000000000007\n",
      "Running apriori with setting: confidence=0.20000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 9\n",
      "Decreasing confidence to 0.15000000000000008\n",
      "Running apriori with setting: confidence=0.15000000000000008, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 10\n",
      "Decreasing confidence to 0.10000000000000007\n",
      "Running apriori with setting: confidence=0.10000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 11\n",
      "Decreasing confidence to 0.05000000000000007\n",
      "Running apriori with setting: confidence=0.05000000000000007, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 12\n",
      "Decreasing confidence to 6.938893903907228e-17\n",
      "Running apriori with setting: confidence=6.938893903907228e-17, support=0.0, minlen=2, maxlen=5, MAX_RULE_LEN=5\n",
      "Rule count: 952, Iteration: 13\n",
      "All options exhausted\n"
     ]
    }
   ],
   "source": [
    "from pyarc.qcba.transformation import QCBATransformation\n",
    "\n",
    "dataset_name = \"iris\"\n",
    "dataset_index = 1\n",
    "\n",
    "\n",
    "\n",
    "train_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)] \n",
    "test_path_undiscr = [ \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "train_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/train/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "test_path_discr = [ \"C:/code/python/machine_learning/assoc_rules/test/{}{}.csv\".format(dataset_name, dataset_index) for dataset_index in range(0, 9)]\n",
    "\n",
    "dataset_train_undiscr = pd.concat([ pd.read_csv(ds) for ds in train_path_undiscr ])\n",
    "dataset_test_undiscr = pd.concat([ pd.read_csv(ds) for ds in test_path_undiscr ])\n",
    "dataset_test_undiscr_Y = dataset_test_undiscr.iloc[:,-1]\n",
    "\n",
    "quant_dataset_train = QuantitativeDataFrame(dataset_train_undiscr)\n",
    "quant_dataset_test = QuantitativeDataFrame(dataset_test_undiscr)\n",
    "\n",
    "txns_train_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in train_path_discr]))\n",
    "txns_test_discr = TransactionDB.from_DataFrame(pd.concat([pd.read_csv(ds) for ds in test_path_discr]))\n",
    "\n",
    "rm_cba = CBA(algorithm=\"m1\", confidence=0.1, support=0.01).fit(txns_train_discr, top_rules_args={\"target_rule_count\":1000})\n",
    "\n",
    "cba_rule_model = rm_cba\n",
    "quantitative_dataset = quant_dataset\n",
    "\n",
    "__quant_rules = [ QuantitativeCAR(r) for r in cba_rule_model.clf.rules ] \n",
    "\n",
    "qcba_transformation = QCBATransformation(quant_dataset_train)\n",
    "\n",
    "\n",
    "refitting=True,\n",
    "literal_pruning=True,\n",
    "trimming=True,\n",
    "extension=True,\n",
    "overlap_pruning=True,\n",
    "transaction_based_drop=True\n",
    "\n",
    "transformation_dict = {\n",
    "    \"refitting\": refitting,\n",
    "    \"literal_pruning\": literal_pruning,\n",
    "    \"trimming\": trimming,\n",
    "    \"extension\": extension,\n",
    "    \"overlap_pruning\": overlap_pruning,\n",
    "    \"transaction_based_drop\": transaction_based_drop\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RulePostPruner:\n",
    "    \n",
    "    def __init__(self, quantitative_dataset):\n",
    "        self.__dataframe = quantitative_dataset\n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        pruned_rules = self.prune(copied_rules)\n",
    "        \n",
    "        return pruned_rules\n",
    "        \n",
    "    def preprocess_dataframe(self):\n",
    "        return self.__dataframe.dataframe.index.values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_most_frequent_class(self):\n",
    "        \"\"\" \n",
    "        requires class column to be the last in dataframe\n",
    "        \n",
    "        gets the most frequent class from dataset\n",
    "        - naive implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        index_counts, possible_classes = pd.factorize(self.__dataframe.dataframe.iloc[:, -1].values)\n",
    "        counts = np.bincount(index_counts)\n",
    "        counts_max = counts.max()\n",
    "        most_frequent_classes = possible_classes[counts == counts_max]\n",
    "        \n",
    "        # return only one\n",
    "        return most_frequent_classes[0], counts_max\n",
    "    \n",
    "    \n",
    "    def get_most_frequent_from_numpy(self, ndarray):\n",
    "        \"\"\"gets a mode from numpy array\n",
    "        \"\"\"\n",
    "        unique, pos = np.unique(ndarray, return_inverse=True) \n",
    "        counts = np.bincount(pos)                  \n",
    "        maxpos = counts.argmax()                      \n",
    "\n",
    "        return (unique[maxpos], counts[maxpos])\n",
    "        \n",
    "    \n",
    "    def find_covered(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def prune(self, rules):\n",
    "        \n",
    "        dataset = self.preprocess_dataframe()\n",
    "        dataset_len = dataset.size\n",
    "        # True if datacase is not covered yet\n",
    "        dataset_mask = [ True ] * dataset_len\n",
    "        \n",
    "        cutoff_rule = rules[-1]\n",
    "        cutoff_class, cutoff_class_count = self.get_most_frequent_class()\n",
    "        \n",
    "        default_class = cutoff_class\n",
    "\n",
    "        total_errors_without_default = 0\n",
    "        \n",
    "        lowest_total_error = dataset_len - cutoff_class_count\n",
    "        \n",
    "        # implement comparators\n",
    "        rules.sort(reverse=True)\n",
    "        \n",
    "        for rule in rules:\n",
    "            covered_antecedent, covered_consequent = self.__dataframe.find_covered_by_rule_mask(rule)\n",
    "\n",
    "            \n",
    "            # dataset -= covered_antecedent\n",
    "            #dataset_mask = dataset_mask & np.logical_not(covered_antecedent)\n",
    "\n",
    "            correctly_covered = covered_antecedent & covered_consequent\n",
    "            \n",
    "            #print(\"correctly covered from mask\", np.sum(correctly_covered & dataset_mask))\n",
    "            \n",
    "            if not any(correctly_covered):\n",
    "                rules.remove(rule)\n",
    "            else:\n",
    "                misclassified = np.sum(covered_antecedent & dataset_mask) - np.sum(correctly_covered & dataset_mask)\n",
    "                \n",
    "                total_errors_without_default += misclassified\n",
    "                \n",
    "                # dataset -= covered_antecedent\n",
    "                #dataset_mask = np.logical_not(dataset_mask & covered_antecedent)\n",
    "                dataset_mask = dataset_mask & np.logical_not(covered_antecedent)\n",
    "\n",
    "\n",
    "                modified_dataset = dataset[dataset_mask]\n",
    "                class_values = self.__dataframe.dataframe.iloc[:,-1][dataset_mask].values\n",
    "                \n",
    "                default_class, default_class_count = self.get_most_frequent_from_numpy(class_values)\n",
    "                \n",
    "                # don't forget to update dataset length\n",
    "                default_rule_error = np.sum(dataset_mask) - default_class_count\n",
    "                total_errors_with_default = default_rule_error + total_errors_without_default\n",
    "                \n",
    "   \n",
    "                \n",
    "                if total_errors_with_default < lowest_total_error:\n",
    "                    cutoff_rule = rule\n",
    "                    lowest_total_error = total_errors_with_default\n",
    "                    cutoff_class = default_class\n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "        \n",
    "        # remove all rules below cutoff rule\n",
    "        index_to_cut = rules.index(cutoff_rule)\n",
    "        rules_pruned = rules[:index_to_cut+1]\n",
    "        \n",
    "        # append new default rule\n",
    "        empty_rule = cutoff_rule.copy()\n",
    "        empty_rule.antecedent = []\n",
    "        empty_rule.consequent = self.__dataframe.dataframe.columns[-1], cutoff_class\n",
    "        \n",
    "        \n",
    "        #rules_pruned.append(empty_rule)\n",
    "        \n",
    "        return rules_pruned, cutoff_class\n",
    "\n",
    "post_pruner = RulePostPruner(quant_dataset_train)\n",
    "    \n",
    "refitted = qcba_transformation.refitter.transform(__quant_rules)\n",
    "literal_pruned = qcba_transformation.literal_pruner.transform(refitted)\n",
    "trimmed = qcba_transformation.trimmer.transform(literal_pruned)\n",
    "#extended = qcba_transformation.extender.transform(trimmed)\n",
    "post_pruned, default_class = post_pruner.transform(trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "program_script = \"\"\"\n",
    "dataset_name = \"segment\"\n",
    "dataset_index = 1\n",
    "\n",
    "train_path_undiscr = \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/train/{}{}.csv\".format(dataset_name, dataset_index) \n",
    "test_path_undiscr = \"C:/code/python/machine_learning/assoc_rules/folds_undiscr/test/{}{}.csv\".format(dataset_name, dataset_index)\n",
    "\n",
    "train_path_discr = \"C:/code/python/machine_learning/assoc_rules/train/{}{}.csv\".format(dataset_name, dataset_index)\n",
    "test_path_discr = \"C:/code/python/machine_learning/assoc_rules/test/{}{}.csv\".format(dataset_name, dataset_index)\n",
    "\n",
    "dataset_train_undiscr = pd.read_csv(train_path_undiscr)\n",
    "dataset_test_undiscr = pd.read_csv(test_path_undiscr)\n",
    "dataset_test_undiscr_Y = dataset_test_undiscr.iloc[:,-1]\n",
    "\n",
    "quant_dataset_train = QuantitativeDataFrame(dataset_train_undiscr)\n",
    "quant_dataset_test = QuantitativeDataFrame(dataset_test_undiscr)\n",
    "\n",
    "txns_train_discr = TransactionDB.from_DataFrame(pd.read_csv(train_path_discr))\n",
    "txns_test_discr = TransactionDB.from_DataFrame(pd.read_csv(test_path_discr))\n",
    "\n",
    "rm_cba = CBA(algorithm=\"m1\", confidence=0.6, support=0.20).fit(txns_train_discr, top_rules_args={\"target_rule_count\":1000})\n",
    "\n",
    "rm_qcba = QCBA(rm_cba, quant_dataset_train)\n",
    "rm_qcba.fit(\n",
    "    refitting=False,\n",
    "    literal_pruning=False,\n",
    "    trimming=False,\n",
    "    extension=True,\n",
    "    overlap_pruning=False,\n",
    "    transaction_based_drop=False\n",
    ")\n",
    "\n",
    "\n",
    "rm_qcba.clf.rule_model_accuracy(quant_dataset_test, dataset_test_undiscr_Y), rm_cba.rule_model_accuracy(txns_test_discr)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "cProfile.run(program_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-07b46936e143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrm_qcba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrm_cba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'rules'"
     ]
    }
   ],
   "source": [
    "len(rm_qcba.clf.rules), len(rm_cba.clf.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class RuleExtender:\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "    \n",
    "        if type(dataframe) != QuantitativeDataFrame:\n",
    "            raise Exception(\n",
    "                \"type of dataset must be pandas.DataFrame\"\n",
    "            )\n",
    "            \n",
    "        self.__dataframe = dataframe\n",
    "        \n",
    "        \n",
    "        \n",
    "    def transform(self, rules):\n",
    "        \n",
    "        copied_rules = [ rule.copy() for rule in rules ]\n",
    "\n",
    "        progress_bar_len = 50\n",
    "        copied_rules_len = len(copied_rules)\n",
    "        progress_bar = \"#\" * progress_bar_len\n",
    "        progress_bar_empty = \" \" * progress_bar_len\n",
    "        last_progress_bar_idx = -1\n",
    "\n",
    "        extended_rules = []\n",
    "\n",
    "        #print(\"len: \", copied_rules_len)\n",
    "\n",
    "        for i, rule in enumerate(copied_rules):\n",
    "            current_progress_bar_idx = math.floor(i / copied_rules_len * progress_bar_len)\n",
    "            \n",
    "            if last_progress_bar_idx != current_progress_bar_idx:\n",
    "                last_progress_bar_idx = current_progress_bar_idx\n",
    "                \n",
    "                progress_string = \"[\" + progress_bar[:last_progress_bar_idx] + progress_bar_empty[last_progress_bar_idx:] + \"]\"\n",
    "                \n",
    "                print(*progress_string, sep=\"\")\n",
    "\n",
    "            extended_rules.append(self.__extend(rule))\n",
    "        \n",
    "        return extended_rules\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __extend(self, rule):\n",
    "        ext = self.__extend_rule(rule)\n",
    "        \n",
    "        return ext\n",
    "        \n",
    "    def __extend_rule(self, rule, min_improvement=0, min_conditional_improvement=-0.01):\n",
    "        \n",
    "        # check improvemnt argument ranges\n",
    "        \n",
    "        current_best = rule\n",
    "        direct_extensions = self.__get_extensions(rule)\n",
    "        \n",
    "        current_best.update_properties(self.__dataframe)\n",
    "        \n",
    "        while True:\n",
    "            extension_succesful = False\n",
    "\n",
    "            direct_extensions = self.__get_extensions(current_best)\n",
    "\n",
    "            #print(\"extending - new cycle\")\n",
    "            \n",
    "            for candidate in direct_extensions:\n",
    "                #print(\"\\tcandidate - direct extensions\")\n",
    "                candidate.update_properties(self.__dataframe)\n",
    "                \n",
    "                delta_confidence = candidate.confidence - current_best.confidence\n",
    "                delta_support = candidate.support - current_best.support\n",
    "                \n",
    "                \n",
    "                if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                    current_best = candidate\n",
    "                    extension_succesful = True\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                if self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                    enlargement = candidate\n",
    "                    \n",
    "                    while True:\n",
    "                        enlargement = self.get_beam_extensions(enlargement)\n",
    "                        \n",
    "                        if not enlargement:\n",
    "                            break\n",
    "                            \n",
    "                        candidate.update_properties(self.__dataframe)\n",
    "                        enlargement.update_properties(self.__dataframe)\n",
    "\n",
    "                        delta_confidence = enlargement.confidence - current_best.confidence\n",
    "                        delta_support = enlargement.support - current_best.support\n",
    "\n",
    "                        if self.__crisp_accept(delta_confidence, delta_support, min_improvement):\n",
    "                            current_best = enlargement\n",
    "                            extension_succesful = True\n",
    "                            \n",
    "                        elif self.__conditional_accept(delta_confidence, min_conditional_improvement):\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            break\n",
    "            \n",
    "            \n",
    "                    if extension_succesful == True:\n",
    "                        break\n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    # continue to next candidate\n",
    "                    continue\n",
    "           \n",
    "        \n",
    "            if extension_succesful == False:\n",
    "                break\n",
    "                    \n",
    "        return current_best\n",
    "        \n",
    "        \n",
    "    def __get_extensions(self, rule):\n",
    "        extended_rules = []\n",
    "        \n",
    "        for literal in rule.antecedent:\n",
    "            attribute, interval = literal\n",
    "            \n",
    "            neighborhood = self.__get_direct_extensions(literal)\n",
    "            \n",
    "            for extended_literal in neighborhood:\n",
    "                # copy the rule so the extended literal\n",
    "                # can replace the default literal\n",
    "                copied_rule = rule.copy()\n",
    "                \n",
    "                # find the index of the literal\n",
    "                # so that it can be replaced\n",
    "                current_literal_index = copied_rule.antecedent.index(literal)\n",
    "                \n",
    "                copied_rule.antecedent[current_literal_index] = extended_literal\n",
    "                copied_rule.was_extended = True\n",
    "                copied_rule.extended_literal = extended_literal\n",
    "                \n",
    "                extended_rules.append(copied_rule)\n",
    "\n",
    "        extended_rules.sort(reverse=True)\n",
    "             \n",
    "        return extended_rules\n",
    "            \n",
    "    \n",
    "    def __get_direct_extensions(self, literal):\n",
    "        \"\"\"\n",
    "        ensure sort and unique\n",
    "        before calling functions\n",
    "        \"\"\"\n",
    "        \n",
    "        attribute, interval = literal\n",
    "\n",
    "        # if nominal\n",
    "        # needs correction to return null and skip when extending\n",
    "        if type(interval) == str:\n",
    "            return [literal]\n",
    "        \n",
    "        vals = self.__dataframe.column(attribute)\n",
    "        vals_len = vals.size\n",
    "\n",
    "        mask = interval.test_membership(vals)\n",
    "\n",
    "        # indices of interval members\n",
    "        # we want to extend them \n",
    "        # once to the left\n",
    "        # and once to the right\n",
    "        # bu we have to check if resulting\n",
    "        # indices are not larger than value size\n",
    "        member_indexes = np.where(mask)[0]\n",
    "\n",
    "        first_index = member_indexes[0]\n",
    "        last_index = member_indexes[-1]\n",
    "\n",
    "        first_index_modified = first_index - 1\n",
    "        last_index_modified = last_index + 1\n",
    "        \n",
    "        no_left_extension = False\n",
    "        no_right_extension = False\n",
    "\n",
    "        if first_index_modified < 0:\n",
    "            no_left_extension = True\n",
    "\n",
    "        # if last_index_modified is larger than\n",
    "        # available indices\n",
    "        if last_index_modified > vals_len - 1:\n",
    "            no_right_extension = True\n",
    "\n",
    "\n",
    "        new_left_bound = interval.minval\n",
    "        new_right_bound = interval.maxval\n",
    "\n",
    "        if not no_left_extension:\n",
    "            new_left_bound = vals[first_index_modified]\n",
    "\n",
    "        if not no_right_extension:\n",
    "            new_right_bound = vals[last_index_modified]\n",
    "\n",
    "\n",
    "        # prepare return values\n",
    "        extensions = []\n",
    "\n",
    "        if not no_left_extension:\n",
    "            # when values are [1, 2, 3, 3, 4, 5]\n",
    "            # and the corresponding interval is (2, 4)\n",
    "            # instead of resulting interval being (1, 4)\n",
    "            \n",
    "            temp_interval = Interval(\n",
    "                new_left_bound,\n",
    "                interval.maxval,\n",
    "                True,\n",
    "                interval.right_inclusive\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        if not no_right_extension:\n",
    "\n",
    "            temp_interval = Interval(\n",
    "                interval.minval,\n",
    "                new_right_bound,\n",
    "                interval.left_inclusive,\n",
    "                True\n",
    "            )\n",
    "\n",
    "            extensions.append((attribute, temp_interval))\n",
    "\n",
    "        return extensions\n",
    "        \n",
    "    \n",
    "    # make private\n",
    "    def get_beam_extensions(self, rule):\n",
    "        if not rule.was_extended:\n",
    "            return None\n",
    "\n",
    "        # literal which extended the rule\n",
    "        literal = rule.extended_literal\n",
    "        \n",
    "        extended_literal = self.__get_direct_extensions(literal)\n",
    "        \n",
    "        if not extended_literal:\n",
    "            return None\n",
    "        \n",
    "        copied_rule = rule.copy()\n",
    "        \n",
    "        literal_index = copied_rule.antecedent.index(literal)\n",
    "        \n",
    "        # so that literal is not an array\n",
    "        copied_rule.antecedent[literal_index] = extended_literal[0]\n",
    "        copied_rule.was_extended = True\n",
    "        copied_rule.extended_literal = extended_literal[0]\n",
    "        \n",
    "        return copied_rule\n",
    "\n",
    "    \n",
    "    \n",
    "    def __crisp_accept(self, delta_confidence, delta_support, min_improvement):\n",
    "        if delta_confidence >= min_improvement and delta_support > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __conditional_accept(self, delta_conf, min_improvement):\n",
    "        if delta_conf >= min_improvement:\n",
    "            return True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyarc.qcba.transformation import *\n",
    "\n",
    "movies_train_undiscr = pd.read_csv(\"../data/movies.csv\", sep=\";\", index_col=0)\n",
    "movies_train_discr = pd.read_csv(\"../data/movies_discr.csv\", sep=\";\", index_col=0)\n",
    "\n",
    "movies_undiscr_txns = movies_train_undiscr.reset_index()\n",
    "movies_discr_txns = TransactionDB.from_DataFrame(movies_train_discr)\n",
    "\n",
    "rm = CBA(algorithm=\"m1\", confidence=0.2, support=0.02).fit(movies_discr_txns)\n",
    "\n",
    "rules = rm.clf.rules\n",
    "\n",
    "quant_dataset = QuantitativeDataFrame(ds)\n",
    "quant_rules = [ QuantitativeCAR(r) for r in rules ]\n",
    "qcba_transformation = QCBATransformation(quant_dataset)\n",
    "#extended_rules = qcba_transformation.extender.transform(quant_rules)\n",
    "quant_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying all transformations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\code\\python\\CBA\\pyarc\\qcba\\data_structures\\quant_dataset.py:272: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask = values == interval\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c06c8c7baa87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformed_rules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdef_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqcba_transformation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_rules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transformation.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, rules, transformation_dict)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"applying all transformations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mrefitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mliteral_pruned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_pruner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefitted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mtrimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrimmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliteral_pruned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mextended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\prune_literals.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, rules)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcopied_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m  \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__trim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcopied_rules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrimmed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\prune_literals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcopied_rules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m  \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__trim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcopied_rules\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrimmed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\transforms\\prune_literals.py\u001b[0m in \u001b[0;36m__trim\u001b[1;34m(self, rule)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mconsequent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsequent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mrule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mdataset_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\data_structures\\quant_rule.py\u001b[0m in \u001b[0;36mupdate_properties\u001b[1;34m(self, quant_dataframe)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0msupport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_rule_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\data_structures\\quant_dataset.py\u001b[0m in \u001b[0;36mcalculate_rule_statistics\u001b[1;34m(self, rule)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[1;31m# this tells us which instances satisfy the literal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mcurrent_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_literal_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliteral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# add cummulated and current mask using logical AND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\code\\python\\CBA\\pyarc\\qcba\\data_structures\\quant_dataset.py\u001b[0m in \u001b[0;36mget_literal_coverage\u001b[1;34m(self, literal, values)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# reshape mask into single dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "transformed_rules, def_class = qcba_transformation.transform(quant_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
